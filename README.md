# FPHA-Afford 
An object affordance dataset for human-object-robot interaction. <br> 

### Paper
  - [FPHA-Afford: A Domain-Specific Benchmark Dataset for Occluded Object Affordance Estimation in Human-Object-Robot Interaction](https://ieeexplore.ieee.org/document/9190733)  
    <details> <summary>Abstract</summary> 
    In human-object-robot interactions, the recent explosion of standard datasets has offered promising opportunities for deep learning techniques in understanding the functionalities of object parts. But most of existing datasets are only suitable for the applications where objects are non-occluded or isolated during interaction while occlusion is a common challenge in practical object affordance estimation task. In this paper, we attempt to address this issue by introducing a new benchmark dataset named FPHA-Afford that is built upon the popular dataset FPHA. In FPHA-Afford, we adopt egocentric-view to pre-process the videos from FPHA and select part of the frames that contain objects under the strong occlusion of hand. To transfer the domain of FPHA into object affordance estimation task, all of the frames are re-annotated with pixel-level affordance masks. In total, our FPHA-Afford collects 61 videos containing 4.3K frames with 6.55K annotated affordance masks belonging to 9 classes. Some of state-of-the-art semantic segmentation architectures are explored and evaluated over FPHA-Afford. We believe the scale, diversity and novelty of our FPHA-Afford could offer great opportunities to researchers in the computer vision community and beyond. Our dataset and experiment code will be made publicly available on https://github.com/Hussainflr/FPHA-Afford < /details> 

### Video 
https://www.youtube.com/watch?v=6mNQ8ukhGoI&t=301s
### Dataset
Please download dataset from google drive link.

 Gdrive: https://drive.google.com/drive/folders/1qj5e3j5K3EIRReCnmiji62vs1M0S6NX9?usp=sharing


### Trained Models Checkpoints 
Trained models checkpoints can be found in following link.<br>
Gdrive: https://drive.google.com/drive/folders/1qj5e3j5K3EIRReCnmiji62vs1M0S6NX9?usp=sharing

### Video 

#### If you find this helpful please consider cite our paper.

@INPROCEEDINGS{9190733,
  author={Hussain.S, S.Muzamil and Liu, Liu and Xu, Wenqiang and Lu, Cewu},
  booktitle={2020 IEEE International Conference on Image Processing (ICIP)}, 
  title={FPHA-Afford: A Domain-Specific Benchmark Dataset for Occluded Object Affordance Estimation in Human-Object-Robot Interaction}, 
  year={2020},
  volume={},
  number={},
  pages={1416-1420},
  doi={10.1109/ICIP40778.2020.9190733}}
